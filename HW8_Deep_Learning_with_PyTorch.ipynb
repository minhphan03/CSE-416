{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJORzpttqKNl"
   },
   "source": [
    "# Homework 8 - Deep Learning\n",
    "\n",
    "\n",
    "This assignment will give you practice using the library PyTorch, which is a popular library for constructing deep learning models. Deep Learning is a very powerful tool because it is able to learn very complicated functions. Deep Learning has revolutionized fields like image and speech recognition. \n",
    "\n",
    "The specific task we are trying to solve in this assignment is image classification. We will be using a very commong dataset called CIFAR-10 that has 60,000 images separated into 10 classes. The classes are \n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "In this assignment, you will practice:\n",
    "* Reading documentation for a modern machine learning library\n",
    "* Writing PyTorch code. \n",
    "* Evaluating neural network models.\n",
    "\n",
    "Fill in the cells provided marked `TODO` with code to answer the questions. **Unless otherwise noted, every answer you submit should have code that clearly shows the answer in the output.** Answers submitted that do not have associated code that shows the answer may not be accepted for credit. \n",
    "\n",
    "**Make sure to restart the kernel and run all cells** (especially before turning it in) to make sure your code runs correctly. Answer the questions on Gradescope and make sure to download this file once you've finished the assignment and upload it to Canvas as well. **Note this assignment takes a long time to run so make sure to do this earlier than later**.\n",
    "\n",
    "> Copyright Â©2021 Hunter Schafer.  All rights reserved.  Permission is hereby granted to students registered for University of Washington CSE/STAT 416 for use solely during Spring Quarter 2021 for purposes of the course.  No other use, copying, distribution, or modification is permitted without prior written consent. Copyrights for third-party components of this work must be honored.  Instructors interested in reusing these course materials should contact the author.\n",
    "\n",
    "---\n",
    "# Step 1 - Background Reading\n",
    "Before starting this assignment, you should familiarize yourself with PyTorch. You should read [their tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) and preferably follow along with a Colab notebook. You should only have to fully read the sections:\n",
    "* \"What is PyTorch?\"\n",
    "* *Optional* \"Autograd: Automatic Differentiation\"\n",
    "* \"Neural Networks\"\n",
    "* \"Training a Classifier\"\n",
    "\n",
    "This should take about an hour (according to the name of the tutorial). Once you are done with that, you should be more ready to tackle this assignment! \n",
    "\n",
    "You will probably also want to look at the [documentation](https://pytorch.org/docs/stable/index.html) to understand what parameters they are specifying in the tutorial.\n",
    "\n",
    "# Assignment\n",
    "**Before you start, make sure you have enabled GPU for your Colab Runtime.** In the Runtime drop down above, click on \"Change runtime type\" and select \"GPU\" in the Hardware Accelerator field.\n",
    "\n",
    "First we import all of the modules we will use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WYYbYF6SqLr0"
   },
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "import matplotlib.pyplot as plt              # For plotting\n",
    "import seaborn as sns                        # For styling plots\n",
    "import torch                                 # Overall PyTorch import\n",
    "import torch.nn as nn                        # For specifying neural networks\n",
    "import torch.nn.functional as F              # For common functions\n",
    "import torch.optim as optim                  # For optimizizing model parameters\n",
    "import torchvision.datasets as datasets      # To download data\n",
    "import torchvision.transforms as transforms  # For pre-processing data\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOmwjAJeqOUg"
   },
   "source": [
    "Then we define some constants that will be used throughout the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tX-q06L7qOmK"
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE    = 3 * 32 * 32   # An image has 32 x 32 pixels each with Red/Green/Blue values. \n",
    "NUM_CLASSES   = 10            # The number of output classes. In this case, from 0 to 9\n",
    "NUM_EPOCHS    = 10            # The number of times we loop over the whole dataset during training\n",
    "BATCH_SIZE    = 100           # The size of input data took for one iteration of an epoch\n",
    "LEARNING_RATE = 1e-3          # The speed of convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgzIWD66qT_0"
   },
   "source": [
    "The first step is to download the dataset using PyTorch. The code below produces a train set and a test set. Some notes on PyTorch terminology:\n",
    "* Most of the time we use the `train_loader` instead of the `trainset`. The reason being the loader gives us batches of examples from the entire `trainset`. Looping over the `train_loader` returns a subset of the examples on each iteration.\n",
    "* PyTorch commonly talks about \"tensors\". For our purposes, a tensor is just a multi-dimensional array. For example, the `trainset` is a tensor with shape `(50000, 32, 32, 3)` because there are 50,000 images, each is 32x32 pixels with 3 color channels (Red / Green / Blue). You can index into these just like `numpy` arrays.\n",
    "* We pre-process the data into tensors and the normalize them so all the pixels have mean 0.5 and standard devaition 0.5.\n",
    "* Training the models on the full dataset can take a long time. We have left some commented out code to help you sample the training/test data to speed up development time. We encourage you to un-comment the code line while you are figuring out how to write the code so it takes less time to run as long as you re-comment the lines out before training your final models.  \n",
    "\n",
    "**IMPORTANT: Make sure you comment the sampling lines out and re-run your solution on the full dataset to get the correct results.** If you do not correctly train the models on the full dataset in your final submission, you will not get full credit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "E32YoWMgqVRe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, \n",
    "                            transform=transform)\n",
    "# Uncomment line below to speed up train time when testing if your code works.\n",
    "# MAKE SURE TO RECOMMENT THE LINE OF CODE BEFORE SUBMITTING. Otherwise your results will be wrong.\n",
    "# trainset, _ = torch.utils.data.random_split(trainset, [BATCH_SIZE * 10, len(trainset) - BATCH_SIZE * 10])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                                           shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, \n",
    "                           transform=transform)\n",
    "# Uncomment line below to speed up train time when testing if your code works.\n",
    "# MAKE SURE TO RECOMMENT THE LINE OF CODE BEFORE SUBMITTING. Otherwise your results will be wrong.\n",
    "# testset, _ = torch.utils.data.random_split(testset, [BATCH_SIZE * 10, len(testset) - BATCH_SIZE * 10])\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, \n",
    "                                          shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su6pFI2EqYCC"
   },
   "source": [
    "----\n",
    "In the cell below, we define the critical helper functions to train and visualize the results of the neural networks. \n",
    "\n",
    "You should read and understand what they are doing since the next step will ask you to modify one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ylUePXoPqZvh"
   },
   "outputs": [],
   "source": [
    "# TODO Edit this cell as described\n",
    "def train(net, train_loader, test_loader,\n",
    "          num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE,\n",
    "          compute_accs=False):\n",
    "    \"\"\"\n",
    "    This function trains the given network on the training data for the given number of epochs.\n",
    "    If compute_accs is true, evaluates the train and test accuracy of the network at the end of\n",
    "    each epoch.\n",
    "\n",
    "    Args:\n",
    "        net: The neural network to train\n",
    "        train_loader, test_loader: The pytorch dataset loaders for the trainst and testset\n",
    "        num_epochs: The number of times to loop over the batches in train_loader\n",
    "        learning_rate: The learning rate for the optimizer\n",
    "        compute_accs: A bool flag for whether or not this function should compute the train and test\n",
    "                      accuracies at the end of each epoch. This feature is useful for visualizing\n",
    "                      how the model is learning, but slows down training time.\n",
    "    Returns:\n",
    "        The train and test accuracies if compute_accs is True, None otherwise\n",
    "    \"\"\"\n",
    "    # First initialize the criterion (loss function) and the optimizer\n",
    "    # (algorithm like gradient descent). Here we use a common loss function for multi-class\n",
    "    # classification called the Cross Entropy Loss and the popular Adam algorithm.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        batch_num = 1\n",
    "        for images, labels in train_loader:   # Loop over each batch in train_loader\n",
    "\n",
    "            # If you are using a GPU, speed up computation by moving values to the GPU\n",
    "            if torch.cuda.is_available():\n",
    "                net = net.cuda()\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "    \n",
    "            optimizer.zero_grad()               # Reset gradient for next computation\n",
    "            outputs = net(images)               # Forward pass: compute the output class given a image\n",
    "            loss = criterion(outputs, labels)   # Compute loss: difference between the pred and true \n",
    "            loss.backward()                     # Backward pass: compute the weight\n",
    "            optimizer.step()                    # Optimizer: update the weights of hidden nodes\n",
    "\n",
    "            if batch_num % 100 == 0:  # Print every 100 batches                              \n",
    "                print(f'Epoch [{epoch}/{num_epochs}], Step [{batch_num}/{len(train_loader)}], '\n",
    "                      f'Loss: {loss.item():.4f}')\n",
    "            \n",
    "            batch_num += 1\n",
    "        \n",
    "        if compute_accs:\n",
    "            train_acc = accuracy(net, train_loader)\n",
    "            test_acc = accuracy(net, test_loader)\n",
    "            train_accs.append(train_acc)\n",
    "            test_accs.append(test_acc)\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Train Accuracy {100 * train_acc:.2f}%, Test Accuracy {100 * test_acc:.2f}%')\n",
    "\n",
    "\n",
    "    if compute_accs:\n",
    "        return train_accs, test_accs\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def accuracy(net, data_loader):\n",
    "    \"\"\"\n",
    "    For a given data_loader, evaluate the model on the dataset and compute its classification\n",
    "    accuracy.\n",
    "\n",
    "    Args:\n",
    "        net: The neural network to train\n",
    "        data_loader: A dataset loader for some dataset.\n",
    "    Returns:\n",
    "        The classificiation accuracy of the model on this dataset.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in data_loader:\n",
    "        if  torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        outputs = net(images)                           # Make predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)       # Choose class with highest scores\n",
    "        total += labels.size(0)                         # Increment the total count\n",
    "        correct += (predicted == labels).sum().item()   # Increment the correct count\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def plot_history(histories):\n",
    "    \"\"\"\n",
    "    Given a series of training/test accuracies from training, plots them to visualize learning.\n",
    "\n",
    "    Args:\n",
    "        histories: A list of dictionaries storing information about each model trained.\n",
    "                   Each dictionary should have the keys:\n",
    "                        * name: The model name\n",
    "                        * train_accs: A list of train accuracies\n",
    "                        * test_accs: A list of test accuracies.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16,10))\n",
    "    epochs = list(range(1, len(histories[0]['train_accs']) + 1))\n",
    "    for model_history in histories:\n",
    "      val = plt.plot(epochs, model_history['test_accs'],\n",
    "                     '--', label=model_history['name'] + ' Test')\n",
    "      plt.plot(epochs, model_history['train_accs'], color=val[0].get_color(),\n",
    "               label=model_history['name'] + ' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlim([1,max(epochs)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7kq5skVqoIH"
   },
   "source": [
    "## Step 2\n",
    "Right now the `train` function works, but we want to add extra functionality so that it keeps track of how the train/test accuracies change as training progresses (if the `compute_accs` parameter is true). It would be very inefficient to evaluate the train/test accuracy at the end of each batch, so instead we should only evaluate it at the end of each training epoch. \n",
    "\n",
    "Modify the code so that at the end of each epoch if `compute_accs` is true, it computes the training and test accuracies and puts those values at the end of  `train_accs` and `test_accs` respectively.\n",
    "\n",
    "To help you understand your model's progress while training, it will also be helpful to include a print statement in the code you write to print out the training/test accuracy when you evaluate it. The print statement can look something like:\n",
    "\n",
    "```python\n",
    "print(f'Epoch [{epoch}/{num_epochs}], Train Accuracy {100 * train_acc:.2f}%, Test Accuracy {100 * test_acc:.2f}%')\n",
    "\n",
    "```\n",
    "\n",
    "---- \n",
    "\n",
    "## Step 3\n",
    "Now with all the helper code done, we are going to actually write the PyTorch code that specifies the neural network models. Each of the models described should be a different Python class with the specified name.\n",
    "\n",
    "### `NetA`\n",
    "The first neural network will be the simplest, in that it has no hidden layers. It should take the image and flatten it to a vector for the input, and then have 10 outputs, one for each class.\n",
    "\n",
    "There should be no non-linearities for this network and is just a very simple linear classifier.\n",
    "\n",
    "\n",
    "### `NetB`\n",
    "The second neural network will be slightly more complicated in that it has a hidden layer with 300 nodes and adds a non-linearity between the layers. It should use the following operations in this order:\n",
    "* Flatten the image to a vector for the input\n",
    "* Use a fully-connected linear layer with 300 hidden-neurons\n",
    "* Use the ReLU activation function\n",
    "* Use a fully-connected linear layer to the 10 outputs.\n",
    "\n",
    "### `NetC`\n",
    "This third neural network will be a convolutional neural network. It should use the following operations in this order: \n",
    "* Use a convolution layer with kernel-width 5 and depth 25\n",
    "* Use the ReLU activation function\n",
    "* Use a max-pool operation with kernel-width 2 and stride 2\n",
    "* Flatten the image to a vector for the next step's input\n",
    "* Use a fully-connected linear layer to the 10 outputs.\n",
    "\n",
    "This architecture can be seen visually in the following diagram (the left-most object is the input image). \n",
    "\n",
    "![Network Architecture](https://courses.cs.washington.edu/courses/cse416/19su/files/assignment-resources/homework/hw7/nn.svg)\n",
    "\n",
    "Notice that these diagrams use the notation \n",
    "```\n",
    "channels @ height x width\n",
    "``` \n",
    "to describe the dimensions of the results at each step.\n",
    "\n",
    "### `NetD`\n",
    "This is your chance to try out some different architectures of your choice. Create your own neural network architecture that has **at least 2 convolution layers** and **at least 2 fully connected layers**. Your goal is to pick a network architecture that achieves at least 65% test accuracy. Most of the points for this problem are based on having a model that meets the minimum layer requirements, with fewer points focusing on the accuracy.\n",
    "\n",
    "*Implementation details/notes*:\n",
    "* For simplicity, your model should be able to be trained with the same learning rate and optimizer as the other models. This is a restriction that is not normal in real-practice, but we want you to follow this to simplify the code and our grading. \n",
    "* Every convolution operation should be followed by a pooling operation. \n",
    "* Every linear layer and convolution layer should have an activation function. However, it is common to not have one on the very last layer to the 10 outputs so you do not need one there.\n",
    "* You will likely want to use `NetC` as a starting point! \n",
    "* It is not required, but it might help to make your code more modular by adding parameters to the constructor to specify numbers that contain details about the architecture. Then you can automate trying many different settings of these hyper-parameters. If you do add these parameters, modify the cell later that produces the training plots to pass in the appropriate parameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "VubkavD_qorw"
   },
   "outputs": [],
   "source": [
    "# TODO Write the network architectures here\n",
    "class NetA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetA, self).__init__()\n",
    "        self.fc = nn.Linear(3*32*32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3*32*32)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NetB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetB, self).__init__()\n",
    "        self.fc1 = nn.Linear(3*32*32, 300)\n",
    "        self.fc2 = nn.Linear(300, 10)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3*32*32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class NetC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetC, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=25, kernel_size=5) # (32-5)/1 + 1 = 28\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # 28/2 = 14\n",
    "        self.fc = nn.Linear(25*14*14, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv(x)))\n",
    "        x = x.view(-1, 25*14*14)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class NetD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetD, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=18, kernel_size=5, padding=2) # (32-5 + 4)/1 + 1 = 32\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # 32/2 = 16\n",
    "        self.drop1 = nn.Dropout(p=0.05)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=18, out_channels=28, kernel_size=5, padding=2) # (16-5+4)/1 + 1 = 16\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # 16/2=8\n",
    "        self.drop2 = nn.Dropout(p=0.05)\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*8*8, 350)\n",
    "        self.fc2 = nn.Linear(350, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.drop1(self.pool1(self.conv1(x))))\n",
    "        x = F.relu(self.drop2(self.pool2(self.conv2(x))))\n",
    "        x = x.view(-1, 28*8*8)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9wjrfwyrQHn"
   },
   "source": [
    "Below is the cell that runs the main part of the code. It trains each of the 4 models you specified above for 10 epochs and then plots their train/test accuracies throughout the training process. When training all 4 models on the entire datasets, this should take about 15 minutes to run. \n",
    "\n",
    "You are welcome to change the `nets` list during development so you don't have to train all 4 models each time, but make sure your final result trains:\n",
    "\n",
    "```\n",
    "nets = [NetA(), NetB(), NetC(), NetD()]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Z9UAYGKUrQjN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Training NetA ====\n",
      "Epoch [1/10], Step [100/500], Loss: 1.8696\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:7\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, train_loader, test_loader, num_epochs, learning_rate, compute_accs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     31\u001b[0m     batch_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:   \u001b[38;5;66;03m# Loop over each batch in train_loader\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m# If you are using a GPU, speed up computation by moving values to the GPU\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     36\u001b[0m             net \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1284\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time \n",
    "nets = [NetA(), NetB(), NetC(), NetD()]\n",
    "histories = []\n",
    "\n",
    "for net in nets:\n",
    "    net_name = type(net).__name__\n",
    "    print(f'==== Training {net_name} ====')\n",
    "    train_history, test_history = train(net, train_loader, test_loader, \n",
    "                                        num_epochs=NUM_EPOCHS, \n",
    "                                        learning_rate=LEARNING_RATE, \n",
    "                                        compute_accs=True)\n",
    "    histories.append({\n",
    "        'name': net_name, \n",
    "        'net': net, \n",
    "        'train_accs': train_history, \n",
    "        'test_accs': test_history\n",
    "    })\n",
    "plot_history(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6b57DX3rT1O"
   },
   "source": [
    "# Before you submit\n",
    "Make sure you have undone any changes to the starter code that impact model training you were not asked to change. Specifically look for the development tips we suggested to undo those changes\n",
    "* Make sure your models are trainied on the whole dataset by re-commenting the sampling lines at earlier in the notebook. Together, the 4 models should take about 15 minutes to train if you are using the whole dataset.\n",
    "* Make you are training all 4 of `NetA`, `NetB`, `NetC`, and `NetD`. Your final plot should show these 4 models' training/test accuracies during over the training phase.\n",
    "* Make sure you have run all the cells and the graph appears in your notebook. For grading, we need to see the desired output in your notebook.\n",
    "\n",
    "# To Submit\n",
    "Download the notebook from Colab as a `.ipynb` notebook (`File > Download > Download .ipynb`) and upload it to the Gradescope assignment for this assignment. Your assignment should be named `HW8-Deep-Learning-with-PyTorch.ipynb`."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW8_Deep_Learning_with_PyTorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
